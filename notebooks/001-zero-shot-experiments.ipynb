{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BARTHubInterface(\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (mnli): BARTClassificationHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model in fairseq\n",
    "import torch\n",
    "import numpy as np\n",
    "from fairseq.models.bart import BARTModel\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "bart = torch.hub.load('pytorch/fairseq', 'bart.large.mnli')\n",
    "bart.eval()  # disable dropout for evaluation\n",
    "bart.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9967863 , 0.00117084, 0.00204286],\n",
       "       [0.0711104 , 0.0326806 , 0.896209  ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode a pair of sentences and make a prediction\n",
    "# 0 = contradiction, 1 = neutral, 2 = entailment\n",
    "batch_of_pairs = [\n",
    "    ['BART is a seq2seq model.', 'BART is not sequence to sequence.'],\n",
    "    ['BART is denoising autoencoder.', 'BART is version of autoencoder.'],\n",
    "]\n",
    "\n",
    "batch = collate_tokens(\n",
    "    [bart.encode(pair[0], pair[1]) for pair in batch_of_pairs], pad_idx=1\n",
    ")\n",
    "logits = bart.predict('mnli', batch).detach().cpu().numpy() \n",
    "probs = np.exp(logits) / np.exp(logits).sum(-1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00358193, 0.5513655 , 0.44505256],\n",
       "       [0.9937598 , 0.00393454, 0.00230566]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more testing\n",
    "batch_of_pairs = [\n",
    "    ['I like Apple', 'Apple is positive'],\n",
    "    ['I hate Apple.', 'Apple is positive'],\n",
    "]\n",
    "\n",
    "batch = collate_tokens(\n",
    "    [bart.encode(pair[0], pair[1]) for pair in batch_of_pairs], pad_idx=1\n",
    ")\n",
    "logits = bart.predict('mnli', batch).detach().cpu().numpy() \n",
    "probs = np.exp(logits) / np.exp(logits).sum(-1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Entity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.756, 0.016, 0.004],\n",
       "       [0.001, 0.013, 0.956],\n",
       "       [0.017, 0.052, 0.307],\n",
       "       [0.666, 0.147, 0.035],\n",
       "       [0.049, 0.052, 0.005]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ZeroShotEntityClassifier:\n",
    "    \"\"\"\n",
    "    Zero Shot Entity Classifier using Facebook's Bart Large model finetuned on MNLI\n",
    "    Classify entities in a given sentence or a batch to labels\n",
    "    \n",
    "    Multiclass, probability calculated across label logits\n",
    "    In case of multilabel, the logits can be calculated across each model output \n",
    "    Model outputs 3 scores: contradiction, neutral and entailment for each sentence, hypothesis pair\n",
    "    \n",
    "    Note and todo: \n",
    "    1. Multilabel probabilities are not implemented\n",
    "    2. Batches are not handled carefully\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,model, finetune=\"mnli\"):\n",
    "        self.model = model\n",
    "        self.finetune=\"mnli\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def get_sentence_pairs(sentences, entities, hypothesis):\n",
    "        return [[\"{}.\".format(s), \"{}.\".format(hypothesis.format(e.title(),l))] for s,en in zip(sentences,entities) for e in en for l in labels]\n",
    "\n",
    "    def prepare_batch(self,sen_hyp_pairs):\n",
    "        return collate_tokens(\n",
    "            [self.model.encode(pair[0], pair[1]) for pair in sen_hyp_pairs], pad_idx=1\n",
    "        )\n",
    "\n",
    "    def get_logits(self,batch, total_entities, multi_label):\n",
    "        # contradiction, neutral and entailment logits for each sentence_hyp_pair\n",
    "        logits = self.model.predict( self.finetune, batch).detach().cpu().numpy()\n",
    "        # we only look at entailment logits, where the hypothesis matches the logic of the sentence\n",
    "        if not multi_label:\n",
    "            entailment_logits = logits[:,-1]\n",
    "            return entailment_logits.reshape(total_entities,len(labels))\n",
    "        else:\n",
    "            return logits.reshape(total_entities,len(labels),-1)\n",
    "             \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_softmax(lgts, multi_label=True):\n",
    "        probs = np.exp(lgts) / np.exp(lgts).sum(-1, keepdims=True)\n",
    "        if multi_label:\n",
    "            probs = probs[:,:,-1] # only entailment logits for multilabel. Already taken care of in case of multiclass\n",
    "        probs = np.around(probs, decimals=3)\n",
    "        return probs\n",
    "        \n",
    "    @staticmethod\n",
    "    def display_results(sentences, entities, probs):\n",
    "        # display probs\n",
    "        prob_index = 0\n",
    "        for s,en in zip(sentences,entities):\n",
    "            print(\"Sentence: {}\".format(s))\n",
    "            print()\n",
    "            for e in en:\n",
    "                print(\"Entity: {}\".format(e))\n",
    "                print( ' | '.join(\"{}:{:.4f}\".format(l,p) for l,p in zip( labels, probs[prob_index])))\n",
    "                print()\n",
    "                prob_index+=1\n",
    "        \n",
    "        \n",
    "    def classify(self, sentences, entities, hypothesis, multi_label=True,print_result=False):\n",
    "        \"\"\"\n",
    "        classify using zero shot and return label probabilities \n",
    "        \"\"\"\n",
    "        # prepare batch\n",
    "        sentence_hyp_pairs = self.get_sentence_pairs(sentences, entities, hypothesis)\n",
    "        batch = self.prepare_batch(sentence_hyp_pairs)\n",
    "        \n",
    "        # get entailment logits\n",
    "        total_entities = sum((len(e) for e in entities))\n",
    "        logits_reshape = self.get_logits(batch,total_entities,multi_label)\n",
    "        probs = self.get_softmax(logits_reshape, multi_label)\n",
    "        \n",
    "        # display results if requested\n",
    "        if print_result:\n",
    "            self.display_results(sentences, entities, probs)\n",
    "\n",
    "        # get the probability across positive, neutral and negative logits\n",
    "        return probs\n",
    "\n",
    "# zero shot sentiment classification using entities\n",
    "labels = [\"positive\",\"neutral\",\"negative\"]\n",
    "hypothesis = \"{} is {}\"\n",
    "sentences = [\"Apple is a great company but iphone sucks\", \"Microsoft xbox is not better than ps4\", \"I shopped at Target\"]\n",
    "entities = [[\"Apple\",\"iphone\"],[\"Microsoft\",\"ps4\"],[\"Target\"]]\n",
    "ZeroShotEntityClassifier(bart).classify(sentences, entities, hypothesis, multi_label=True,print_result=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Entity Sentiment Analysis using Entailment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter api and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "from string import printable, punctuation\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "from twitter_api import Twitter\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "REMOVE_TAGS = [\"rt\"]\n",
    "link_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "hashtag_regex = r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))#([A-Za-z]+[A-Za-z0-9]+)'\n",
    "\n",
    "    \n",
    "class TweetPreprocessor:\n",
    "    \"\"\" preprocessing code to tokenize tweets\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tw_api = Twitter()\n",
    "        self.tknzr = TweetTokenizer(strip_handles=False)\n",
    "        self.tokenize_and_filter = lambda tweet: [t for t in self.tnzr.tokenize(tweet)\n",
    "                                                  if not any(r in t.lower() for r in self.remove_tags)]\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def return_case_insen_keyword(string, keyword):\n",
    "        r= re.search(r'\\b({})\\b'.format(keyword), string,re.IGNORECASE)\n",
    "            \n",
    "        return string[r.start():r.start()+len(keyword)] if r else keyword.title()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_links(text):\n",
    "        return re.sub(link_regex, ' ', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_hashtags(text):\n",
    "        return re.sub(hashtag_regex, ' ', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_emoji(character):\n",
    "        return character in UNICODE_EMOJI\n",
    "\n",
    "    @staticmethod\n",
    "    def is_punct(character):\n",
    "        return character in punctuation\n",
    "\n",
    "    @classmethod\n",
    "    def cleanup_non_alpha(cls, tokens):\n",
    "        prev_char = '329'  # random alphanum string\n",
    "        first_alpha = False\n",
    "        new_tokens = []\n",
    "        for i in tokens:\n",
    "            is_char_alpha = i.isalnum()\n",
    "            is_char_punct = cls.is_punct(i)\n",
    "            is_char_emoji = cls.is_emoji(i)\n",
    "\n",
    "            # remove character if its a punctuation (other than quotations)\n",
    "            if is_char_alpha:\n",
    "                first_alpha = True\n",
    "\n",
    "            if (not first_alpha) and (i not in '\"\\'') and is_char_punct:\n",
    "                continue\n",
    "\n",
    "            # remove repeating punctuations or emojis\n",
    "            if is_char_emoji or is_char_punct:\n",
    "                if i == prev_char:\n",
    "                    continue\n",
    "                prev_char = i\n",
    "\n",
    "            # check remove tags\n",
    "            if i.lower().strip() in REMOVE_TAGS:\n",
    "                continue\n",
    "\n",
    "            # if the character is outside alpha\n",
    "            if not (is_char_alpha or is_char_punct or is_char_emoji) and len(i) == 1:\n",
    "                continue\n",
    "\n",
    "            new_tokens.append(i)\n",
    "        return new_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_case(tokens):\n",
    "        \"\"\"convert tweets with all or mostly upper case to all lower case\"\"\"\n",
    "        num_tokens = sum([1 for t in tokens if t.isalpha()])\n",
    "        if not num_tokens:\n",
    "            return []\n",
    "        num_caps = sum([1 for t in tokens if (t.isupper() or t.istitle())])\n",
    "        return [t.lower() for t in tokens] if num_caps / num_tokens > 0.7 else tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_headline(tweet):\n",
    "        \"\"\"\n",
    "        check whether a tweet is a headline e.g 'Donald Trump Has Started Trade Wars'\n",
    "        if the number of title words in the tweet is > 80% of total words\n",
    "        \"\"\"\n",
    "        sen_splits = tweet.split()\n",
    "        num_title = len([s for s in sen_splits if s.istitle()])\n",
    "        return (num_title / len(sen_splits)) > 0.8\n",
    "\n",
    "    @staticmethod\n",
    "    def strip_non_alpha_and_lower(tweet):\n",
    "        \"\"\"used for near duplicate elimination\"\"\"\n",
    "        return ' '.join([t.lower() for t in tweet.split() if t.isalpha()][:10])\n",
    "\n",
    "    def preprocess_tweet(self, tweet, min_alpha_token_length=3, remove_emoticons=False, keep_hashtags=True):\n",
    "        \"\"\"\n",
    "        filter tweet by,\n",
    "        1. Removing hashtags, at mentions and urls\n",
    "        2. combining repeating punctuations and emoticons into one\n",
    "        3. if the number of alphabetic words are less than min_alpha_token_length, remove the tweet\n",
    "        4. if the tweet has all caps or all title case words, make them all lower\n",
    "        5. replace \"'s\" with \" 's\"\n",
    "        \"\"\"\n",
    "        tweet = unidecode(tweet)\n",
    "        tweet = tweet.replace(\"\\n\", \" \")\n",
    "        if remove_emoticons:\n",
    "            tweet = self.remove_non_ascii(tweet)\n",
    "        if not keep_hashtags:\n",
    "            tweet = self.remove_hashtags(tweet)\n",
    "        tweet = self.remove_links(tweet)\n",
    "        tokens = self.tknzr.tokenize(tweet)\n",
    "        tokens = self.cleanup_non_alpha(tokens)\n",
    "        tokens = self.normalize_case(tokens)\n",
    "        num_alpha_tokens = sum([1 for t in tokens if t.isalpha()])\n",
    "        if num_alpha_tokens < min_alpha_token_length:\n",
    "            return ''\n",
    "        return ' '.join(tokens).replace(\"'s\", \" 's\")\n",
    "    \n",
    "    def get_cleaned_tweets(self, search_term,entity_term, count):\n",
    "        tweets = self.tw_api.get_tweets(search_term, count)\n",
    "        # preprocess and filter tweets with the actual entity term, not present as a subset of a larger word\n",
    "        tweets = [self.preprocess_tweet(t) for t in tweets]\n",
    "        tweets = [t for t in tweets if t and any(search_term.lower()==w for w in t.lower().split())]\n",
    "        # keywords with cases as they appear in tweets\n",
    "        keywords = [self.return_case_insen_keyword(t, entity_term) for t in tweets]\n",
    "        return tweets, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_tweet_around_entity(tweet, entity_term, sen_len_offset = 10):\n",
    "    tw_splits = tweet.split()\n",
    "    en_index = tw_splits.index(entity_term)\n",
    "    start_index = max(0,en_index-sen_len_offset)\n",
    "    end_index = min(len(tw_splits),en_index+sen_len_offset)\n",
    "    tw_chopped = ' '.join(tw_splits[start_index:end_index])\n",
    "    # monitoring token_len to be less than 40 to fit into the GPU of G4dnx with batch size 8 while running inference\n",
    "    token_len = collate_tokens([bart.encode(tw_chopped,hypothesis.format(entity_term,\"positive\"))],pad_idx=1).shape[1]\n",
    "    return chop_tweet_around_entity(tweet, entity_term, sen_len_offset-1) if token_len > 40 else tw_chopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 requests and 2 15 min sleeps required\n",
      "Max requests per window reached. Sleeping ...\n",
      "Max requests per window reached. Sleeping ...\n",
      "Time Taken 1949.4088015556335 Seconds\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "#sleep(15*60)\n",
    "# to rest the api window\n",
    "\n",
    "tp = TweetPreprocessor()\n",
    "\n",
    "data_dir = \"data\"\n",
    "search_term = \"@amazon\"\n",
    "entity_term = \"amazon\"\n",
    "count = 50000\n",
    "\n",
    "\n",
    "csv_file = \"data/{}_{}.csv\".format(search_term.lower(), count)\n",
    "if path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[~df.isnull()['tweets']] # remove nans\n",
    "    tweets, keywords = df['tweets'], df['entities']\n",
    "else:\n",
    "    tweets, keywords = tp.get_cleaned_tweets(search_term, entity_term, count)\n",
    "    pd.DataFrame({'tweets':tweets,'entities':keywords}).to_csv(csv_file,index=False)\n",
    "    \n",
    "# replace at mention with the entity term\n",
    "tweets = [re.sub(search_term, entity_term, t, flags=re.IGNORECASE) for t in tweets]\n",
    "tweets = [chop_tweet_around_entity(t,entity_term,15) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst1,lst2, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst1), n):\n",
    "        yield lst1[i:i + n],lst2[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5973it [30:20,  3.28it/s]                          \n"
     ]
    }
   ],
   "source": [
    "labels = [\"positive\",\"neutral\",\"negative\"]\n",
    "hypothesis = \"{} is {}\"\n",
    "probs_all = []\n",
    "chunk_size = 8\n",
    "zc = ZeroShotEntityClassifier(bart)\n",
    "entities = [[e] for e in keywords]\n",
    "from tqdm import tqdm\n",
    "for t,e in tqdm(chunks(tweets,entities,chunk_size),total=int(len(tweets)/chunk_size)):\n",
    "    \n",
    "    # get classification probs\n",
    "    probs = zc.classify(t,e, hypothesis,multi_label=True, print_result=False)\n",
    "    probs_all.extend(probs)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/share/virtualenvs/fairseq-fsc_dnjc/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It 's Sunday ... let 's turn up the VOLUME @NicoleBrizee amazon amazonlaunchpad</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fixed that for you amazon amazonUK amazonHelp with thanks to @SavingDowns1 . Hope this cheers you up @AmazingBrent</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A friend of @ciarale01 has had this very human response from amazon</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the art and craft of writing christian fiction : #religion #christianfiction #sponsored amazon</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi amezon win me prize Today best Contest mi notebook @MukeshK41711839 amazonInQuiz @aqt11u amazon</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47773</th>\n",
       "      <td>fox news . so should their advertisers amazon @zappos @audible_com @pillpack @ring @adt @tecovas</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47774</th>\n",
       "      <td>amazon to boost hiring , will host upcoming Career Day . #retail #ecommerce #amazon #housewares</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47775</th>\n",
       "      <td>Last week amazon was caught trying to hire former private military contractors to spy on workers and</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47776</th>\n",
       "      <td>Last week amazon was caught trying to hire former private military contractors to spy on workers and</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47777</th>\n",
       "      <td>@athenaforall : Last week amazon was caught trying to hire former private military contractors to spy on workers and</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47778 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     tweets  \\\n",
       "0      It 's Sunday ... let 's turn up the VOLUME @NicoleBrizee amazon amazonlaunchpad                                        \n",
       "1      Fixed that for you amazon amazonUK amazonHelp with thanks to @SavingDowns1 . Hope this cheers you up @AmazingBrent     \n",
       "2      A friend of @ciarale01 has had this very human response from amazon                                                    \n",
       "3      the art and craft of writing christian fiction : #religion #christianfiction #sponsored amazon                         \n",
       "4      Hi amezon win me prize Today best Contest mi notebook @MukeshK41711839 amazonInQuiz @aqt11u amazon                     \n",
       "...                                                                                                   ...                     \n",
       "47773  fox news . so should their advertisers amazon @zappos @audible_com @pillpack @ring @adt @tecovas                       \n",
       "47774  amazon to boost hiring , will host upcoming Career Day . #retail #ecommerce #amazon #housewares                        \n",
       "47775  Last week amazon was caught trying to hire former private military contractors to spy on workers and                   \n",
       "47776  Last week amazon was caught trying to hire former private military contractors to spy on workers and                   \n",
       "47777  @athenaforall : Last week amazon was caught trying to hire former private military contractors to spy on workers and   \n",
       "\n",
       "       Positive  Neutral  Negative  \n",
       "0      0.164     0.000    0.001     \n",
       "1      0.074     0.001    0.001     \n",
       "2      0.104     0.001    0.023     \n",
       "3      0.057     0.000    0.000     \n",
       "4      0.286     0.000    0.000     \n",
       "...      ...       ...      ...     \n",
       "47773  0.049     0.001    0.004     \n",
       "47774  0.548     0.000    0.000     \n",
       "47775  0.000     0.000    0.336     \n",
       "47776  0.000     0.000    0.336     \n",
       "47777  0.000     0.000    0.380     \n",
       "\n",
       "[47778 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "sentiment_df = pd.DataFrame(probs_all,columns=[\"Positive\",\"Neutral\",\"Negative\"])\n",
    "sentiment_df.insert(0,\"tweets\",tweets)\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4336</th>\n",
       "      <td>amazon cheers for that</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33737</th>\n",
       "      <td>The Real Napoleon : The Untold Story by John Tarttelin via amazon \" Excellent read , I can't put this book down \" #paperback #book</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>The Real Napoleon : The Untold Story by John Tarttelin via amazon \" Excellent read , I can't put this book down \" #paperback #book</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>amazon cheers for that</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17945</th>\n",
       "      <td>@ciarale01 amazon That 's great news . I'd like to see spreading hatred against disability become</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17984</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30311</th>\n",
       "      <td>the like should be knocking on my social media . The amazon reviews have been lovely so far #comedy #books #dvd #comedybook</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30313</th>\n",
       "      <td>the like should be knocking on my social media . The amazon reviews have been lovely so far #comedy #books #dvd #comedybook</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15205</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15497</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11949</th>\n",
       "      <td>@ana_couper amazon Congratulations ! Great job What an accomplishment</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33742</th>\n",
       "      <td>Real Napoleon - The Untold Story by John Tarttelin via amazon \" Impressive , powerful convincing \" #kindle #napoleon #history</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24019</th>\n",
       "      <td>who knows ! His novel Suitcase Charlie is great #writing #writers #BookRecommendation #BookReview Read the amazon reviews They concur</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20359</th>\n",
       "      <td>who knows ! His novel Suitcase Charlie is great #writing #writers #BookRecommendation #BookReview Read the amazon reviews They concur</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16387</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Praise where praise is due . Well done Amazon</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16432</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Praise where praise is due . Well done Amazon</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>Positive Review . . . The Lottery Cypher by George R . Martin III via amazon</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47502</th>\n",
       "      <td>as @Microsoft leapt 4.3 % , @Apple jumped 4 % , amazon surged 3.8 % , @Twitter climbed 3.6 % , @Google</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30694</th>\n",
       "      <td>@producerhoss amazon cheers ! white paw brand two pack funny dog drinks</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17747</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Looks very promising , certainly better than the usual generic replies .</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Looks very promising , certainly better than the usual generic replies .</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13387</th>\n",
       "      <td>@JacquinNugent amazon Great news . I hope it sells like hot cakes and hot cross buns</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17381</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Fantastic response from Amazon .</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>@ciarale01 amazon Hooray ! Thank goodness Ciara Well done for highlighting this . Hope it doesn't</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21630</th>\n",
       "      <td>@ciarale01 amazon Hooray ! Thank goodness Ciara Well done for highlighting this . Hope it doesn't</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17299</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Fantastic response from Amazon .</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47318</th>\n",
       "      <td>Always great to get great reviews on amazon #amazonreview #5stars #foamrolling #Fitfam #foamroller</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36699</th>\n",
       "      <td>@MitziSzereto @truecrimez_com amazon @MangoPublishing @PodiumAudio Wow ... well done you ! Major ' happy</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13395</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Good , well done all !</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13450</th>\n",
       "      <td>@sallyephillips @ciarale01 amazon Good , well done all !</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32397</th>\n",
       "      <td>amazon This is great news</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28390</th>\n",
       "      <td>amazon way to go no-fap February . get it</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4333</th>\n",
       "      <td>amazon for the win PERIODT</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28389</th>\n",
       "      <td>amazon way to go no-fap February . get it</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28391</th>\n",
       "      <td>amazon way to go no-fap February . get it</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>the wealth race for another 2 decades . Win Win for [ ? ] amazon [ ? ] [ ? ] @Verizon [ ? ] [ ? ]</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26529</th>\n",
       "      <td>Sunday Times Bestseller and Richard &amp; Judy Book Club Pick by Lefteri , Christy via amazon \" Moving , powerful compassionate and beautifully written ... a testament to the triumph</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11882</th>\n",
       "      <td>. \" 5 stars ! Picture Imperfect : A Sassy Suspense by Cindy Procter-King via amazon</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21166</th>\n",
       "      <td>@ViolinMonster @Naked_Determina @RashidaTlaib @DebDingell amazon @313Supa All good , didn't</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14325</th>\n",
       "      <td>@JenBullock22 Does he like dinosaurs ? Outer space The ocean If so , amazon for the win !</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15116</th>\n",
       "      <td>@JarredMcGinnis @SimonOldfield @ashortaffair amazon Yay ! Me too</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40261</th>\n",
       "      <td>@1jacqie1 @echovectorbravo @elgato amazon nice ! congrats jacqie</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22116</th>\n",
       "      <td>@ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>amazon This is great friends ) I will be happy if you mention my engineering</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22658</th>\n",
       "      <td>@ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22485</th>\n",
       "      <td>@ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22141</th>\n",
       "      <td>@ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17916</th>\n",
       "      <td>@ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   tweets  \\\n",
       "4336   amazon cheers for that                                                                                                                                                               \n",
       "33737  The Real Napoleon : The Untold Story by John Tarttelin via amazon \" Excellent read , I can't put this book down \" #paperback #book                                                   \n",
       "4997   The Real Napoleon : The Untold Story by John Tarttelin via amazon \" Excellent read , I can't put this book down \" #paperback #book                                                   \n",
       "4339   amazon cheers for that                                                                                                                                                               \n",
       "17945  @ciarale01 amazon That 's great news . I'd like to see spreading hatred against disability become                                                                                    \n",
       "18003  @sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to                                                                        \n",
       "17984  @sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to                                                                        \n",
       "17999  @sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to                                                                        \n",
       "30311  the like should be knocking on my social media . The amazon reviews have been lovely so far #comedy #books #dvd #comedybook                                                          \n",
       "30313  the like should be knocking on my social media . The amazon reviews have been lovely so far #comedy #books #dvd #comedybook                                                          \n",
       "15205  @sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to                                                                        \n",
       "15497  @sallyephillips @ciarale01 amazon This is very positive encouraging from amazon and it needs to be applied to                                                                        \n",
       "11949  @ana_couper amazon Congratulations ! Great job What an accomplishment                                                                                                                \n",
       "33742  Real Napoleon - The Untold Story by John Tarttelin via amazon \" Impressive , powerful convincing \" #kindle #napoleon #history                                                        \n",
       "24019  who knows ! His novel Suitcase Charlie is great #writing #writers #BookRecommendation #BookReview Read the amazon reviews They concur                                                \n",
       "20359  who knows ! His novel Suitcase Charlie is great #writing #writers #BookRecommendation #BookReview Read the amazon reviews They concur                                                \n",
       "16387  @sallyephillips @ciarale01 amazon Praise where praise is due . Well done Amazon                                                                                                      \n",
       "16432  @sallyephillips @ciarale01 amazon Praise where praise is due . Well done Amazon                                                                                                      \n",
       "8967   Positive Review . . . The Lottery Cypher by George R . Martin III via amazon                                                                                                         \n",
       "47502  as @Microsoft leapt 4.3 % , @Apple jumped 4 % , amazon surged 3.8 % , @Twitter climbed 3.6 % , @Google                                                                               \n",
       "30694  @producerhoss amazon cheers ! white paw brand two pack funny dog drinks                                                                                                              \n",
       "17747  @sallyephillips @ciarale01 amazon Looks very promising , certainly better than the usual generic replies .                                                                           \n",
       "17850  @sallyephillips @ciarale01 amazon Looks very promising , certainly better than the usual generic replies .                                                                           \n",
       "13387  @JacquinNugent amazon Great news . I hope it sells like hot cakes and hot cross buns                                                                                                 \n",
       "17381  @sallyephillips @ciarale01 amazon Fantastic response from Amazon .                                                                                                                   \n",
       "21243  @ciarale01 amazon Hooray ! Thank goodness Ciara Well done for highlighting this . Hope it doesn't                                                                                    \n",
       "21630  @ciarale01 amazon Hooray ! Thank goodness Ciara Well done for highlighting this . Hope it doesn't                                                                                    \n",
       "17299  @sallyephillips @ciarale01 amazon Fantastic response from Amazon .                                                                                                                   \n",
       "47318  Always great to get great reviews on amazon #amazonreview #5stars #foamrolling #Fitfam #foamroller                                                                                   \n",
       "36699  @MitziSzereto @truecrimez_com amazon @MangoPublishing @PodiumAudio Wow ... well done you ! Major ' happy                                                                             \n",
       "13395  @sallyephillips @ciarale01 amazon Good , well done all !                                                                                                                             \n",
       "13450  @sallyephillips @ciarale01 amazon Good , well done all !                                                                                                                             \n",
       "32397  amazon This is great news                                                                                                                                                            \n",
       "28390  amazon way to go no-fap February . get it                                                                                                                                            \n",
       "4333   amazon for the win PERIODT                                                                                                                                                           \n",
       "28389  amazon way to go no-fap February . get it                                                                                                                                            \n",
       "28391  amazon way to go no-fap February . get it                                                                                                                                            \n",
       "28795  the wealth race for another 2 decades . Win Win for [ ? ] amazon [ ? ] [ ? ] @Verizon [ ? ] [ ? ]                                                                                    \n",
       "26529  Sunday Times Bestseller and Richard & Judy Book Club Pick by Lefteri , Christy via amazon \" Moving , powerful compassionate and beautifully written ... a testament to the triumph   \n",
       "11882  . \" 5 stars ! Picture Imperfect : A Sassy Suspense by Cindy Procter-King via amazon                                                                                                  \n",
       "21166  @ViolinMonster @Naked_Determina @RashidaTlaib @DebDingell amazon @313Supa All good , didn't                                                                                          \n",
       "14325  @JenBullock22 Does he like dinosaurs ? Outer space The ocean If so , amazon for the win !                                                                                            \n",
       "15116  @JarredMcGinnis @SimonOldfield @ashortaffair amazon Yay ! Me too                                                                                                                     \n",
       "40261  @1jacqie1 @echovectorbravo @elgato amazon nice ! congrats jacqie                                                                                                                     \n",
       "22116  @ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface                                                    \n",
       "4739   amazon This is great friends ) I will be happy if you mention my engineering                                                                                                         \n",
       "22658  @ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface                                                    \n",
       "22485  @ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface                                                    \n",
       "22141  @ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface                                                    \n",
       "17916  @ciarale01 amazonUK Yay . Well done all and thank you amazon - now wondering how you are going to make sure they do not resurface                                                    \n",
       "\n",
       "       Positive  Neutral  Negative  \n",
       "4336   0.985     0.001    0.000     \n",
       "33737  0.985     0.001    0.000     \n",
       "4997   0.985     0.001    0.000     \n",
       "4339   0.985     0.001    0.000     \n",
       "17945  0.983     0.003    0.001     \n",
       "18003  0.981     0.001    0.000     \n",
       "17984  0.981     0.001    0.000     \n",
       "17999  0.981     0.001    0.000     \n",
       "30311  0.981     0.001    0.001     \n",
       "30313  0.981     0.001    0.001     \n",
       "15205  0.981     0.001    0.000     \n",
       "15497  0.981     0.001    0.000     \n",
       "11949  0.980     0.000    0.000     \n",
       "33742  0.980     0.000    0.000     \n",
       "24019  0.979     0.000    0.000     \n",
       "20359  0.979     0.000    0.000     \n",
       "16387  0.978     0.001    0.000     \n",
       "16432  0.978     0.001    0.000     \n",
       "8967   0.977     0.001    0.000     \n",
       "47502  0.977     0.000    0.001     \n",
       "30694  0.976     0.000    0.000     \n",
       "17747  0.976     0.001    0.000     \n",
       "17850  0.976     0.001    0.000     \n",
       "13387  0.976     0.001    0.000     \n",
       "17381  0.975     0.001    0.000     \n",
       "21243  0.975     0.000    0.000     \n",
       "21630  0.975     0.000    0.000     \n",
       "17299  0.975     0.001    0.000     \n",
       "47318  0.975     0.000    0.000     \n",
       "36699  0.974     0.000    0.001     \n",
       "13395  0.974     0.001    0.001     \n",
       "13450  0.974     0.001    0.001     \n",
       "32397  0.974     0.001    0.000     \n",
       "28390  0.974     0.000    0.001     \n",
       "4333   0.974     0.001    0.001     \n",
       "28389  0.974     0.000    0.001     \n",
       "28391  0.974     0.000    0.001     \n",
       "28795  0.974     0.001    0.000     \n",
       "26529  0.973     0.000    0.000     \n",
       "11882  0.973     0.001    0.001     \n",
       "21166  0.972     0.129    0.002     \n",
       "14325  0.972     0.001    0.001     \n",
       "15116  0.971     0.001    0.000     \n",
       "40261  0.970     0.001    0.000     \n",
       "22116  0.970     0.000    0.000     \n",
       "4739   0.970     0.002    0.000     \n",
       "22658  0.970     0.000    0.000     \n",
       "22485  0.970     0.000    0.000     \n",
       "22141  0.970     0.000    0.000     \n",
       "17916  0.970     0.000    0.000     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df[sentiment_df.Positive/sentiment_df.Negative > 100].sort_values('Positive',ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df[(sentiment_df.Negative<0.01) & (sentiment_df.Positive>0.2)].sort_values('Positive',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.Negative.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: REMOVE CUSTOM TOKENIZATION OR KEEP IT MINIMAL. USE THE MODEL'S PREPROCESSING LOGIC (collate_tokens?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp([-3.10656931e-02, -3.50330782e+00, -7.62020540e+00]) / np.exp([-3.10656931e-02, -3.50330782e+00, -7.62020540e+00]).sum(-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Notes:\n",
    "1. Need multilabel approach. Separate 0-1 for each positive, negative and neutral labels by using softmax across model output for each sen, hyp pairs\n",
    "2. Use the model \"neutral\" output for sentiment \"neutral\" label?\n",
    "3. Multilabel sometimes giving low scores. Softmax across cont, neut, ent for each label\n",
    "4. Experiment with different hypothesis\n",
    "5. Effect of hashtags and at mentions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
